# MATH-OF-DEEP-LEARNING
COMPARE SHALLOW AND DEEP NEURAL NETWORKS FOR FUNCTION APPROXIMATION
Based on the paper titled “Why Deep Neural Networks for Function
Approximation?” by Shiyu Liang and R. Srikant [1]
, we design experiments to prove
the theorems and results given in the paper. First, we consider univariate functions
on a bounded interval and use a neural network to achieve an approximation error
of ε uniformly over the interval. We show that for a given structure in the paper, the
mean absolute error is less than or equal to ε. And with the same number of
neurons, we compare the mean absolute error of deep networks and shallow
networks and show that the performance of deep networks is better than shallow
networks. We then extend these results to multivariate functions. Our results are
derived from neural networks using fully-connected rectifier linear units (ReLUs)
networks as our architecture.

